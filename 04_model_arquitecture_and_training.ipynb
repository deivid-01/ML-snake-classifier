{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_model_arquitecture_and_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CmweXhcOJni3"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8jzgA5QKXVe"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deivid-01/Snakes-Classifier/blob/main/04_model_arquitecture_and_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLj9TpczITy9",
        "tags": []
      },
      "source": [
        "# Model arquitecture and training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7irW4YyfLMM2"
      },
      "source": [
        "### About:\n",
        "- This notebook focus on train and validate machine learning model.\n",
        "---\n",
        "David Andr√©s Torres Betancour <br/>\n",
        "Computer Engineering  Student <br/>\n",
        "University of Antioquia <br/>\n",
        "davida.torres@udea.edu.co"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwTmy6SpJP1I"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fFAqu659xSp"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Flatten, Dense,Dropout\n",
        "from tensorflow.keras.models  import Model,load_model\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "#from keras.applications import vgg16\n",
        "from tensorflow.keras.applications import VGG16 # Wtf ?\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from google.colab import files\n",
        "import h5py\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0seoI_clkGrL"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PUsnxgM4CNu"
      },
      "source": [
        "### Kaggle Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eugra7qpkIzR"
      },
      "source": [
        "class bcolors:\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "\n",
        "def loadCredentials():\n",
        "  '''\n",
        "  Description:\n",
        "    Loads and configure kaggle.json file with credentials.\n",
        "\n",
        "  Parameters:\n",
        "      No parameters.\n",
        "\n",
        "  Returns:\n",
        "      No return.   \n",
        "  ''' \n",
        "  print(bcolors.BOLD + \"Upload kaggle.json file with credentials\\n\" + bcolors.ENDC)\n",
        "  print(bcolors.WARNING + \"IMPORTANT: Filename must be: kaggle.json\\n\" + bcolors.ENDC)\n",
        "  uploaded_file=files.upload() #Upload file\n",
        "  assert len(uploaded_file)>0,\"Fetch canceled: Dataset not loaded\"\n",
        "  assert list(uploaded_file.keys())[0]=='kaggle.json', \"Filename must be: kaggle.json\"\n",
        " \n",
        "\n",
        "\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = '/content' #Setup folder\n",
        "  ! chmod 600 /content/kaggle.json  #Hide kaggle.json\n",
        "\n",
        "\n",
        "def fetchDatasetFromKaggle(dataset_name, force_fetch=False):\n",
        "  '''\n",
        "  Description:\n",
        "    fetchs dataset  from kaggle.\n",
        "\n",
        "  Parameters:\n",
        "      No parameters.\n",
        "\n",
        "  Returns:\n",
        "      No return.   \n",
        "  '''\n",
        "  if os.path.exists('kaggle.json') == False: \n",
        "    loadCredentials() \n",
        "\n",
        "  if force_fetch==True:\n",
        "   process_info=! kaggle datasets download -d deividt/{dataset_name} --force #Download dataset\n",
        "   if \"100%\" in list(process_info)[-1]:\n",
        "       print(bcolors.OKGREEN + \"Data from kaggle successfully fetch\\n\" + bcolors.ENDC)\n",
        "       print(bcolors.BOLD + \"Unzipping data...\" + bcolors.ENDC)\n",
        "       ! unzip \\*.zip && rm *.zip\n",
        "       print(bcolors.OKGREEN + \"Data is ready in your local folder!\\n\" + bcolors.ENDC)\n",
        "   elif \"404 - Not Found\" in list(process_info)[0]: \n",
        "       print(bcolors.FAIL + \"404 - Dataset  Not Found in 'deividt' Account\\n\" + bcolors.ENDC)\n",
        "   else:\n",
        "       assert False,list(process_info)[-1]\n",
        "\n",
        "\n",
        "  else:\n",
        "   process_info =  ! kaggle datasets download -d deividt/{dataset_name} \n",
        "   if \"Skipping\" in list(process_info)[0]:\n",
        "       print(bcolors.WARNING + \"Data already exists locally\\nIf you want force fetch set force_fetch parameter to True\" + bcolors.ENDC)\n",
        "  \n",
        "   elif \"100%\" in list(process_info)[-1]:\n",
        "       print(bcolors.OKGREEN + \"Data from kaggle successfully fetch\\n\" + bcolors.ENDC)\n",
        "       print(bcolors.BOLD + \"Unzipping data... \\n\" + bcolors.ENDC)\n",
        "       ! unzip \\*.zip && rm *.zip #Unzip downloaded file and delete zip\n",
        "       print(bcolors.OKGREEN + \"Data is ready in your local folder!\\n\" + bcolors.ENDC)\n",
        "   elif \"404 - Not Found\" in list(process_info)[0]: \n",
        "       print(bcolors.FAIL + \"404 - Dataset  Not Found in 'deividt' Account\\n\" + bcolors.ENDC)\n",
        "   else:\n",
        "       assert 1==2,list(process_info)[-1]\n",
        "\n",
        "\n",
        "\n",
        "def fetchModelsFromKaggle():\n",
        "  ! kaggle datasets download -d deividt/snake-classifier-pretrainedmodels #Download dataset\n",
        "  ! unzip \\*.zip && rm *.zip #Unzip downloaded file and delete zip\n",
        "\n",
        "def kaggle_createDataset(public=False):\n",
        "  if public: \n",
        "    !kaggle datasets create -p  /content/model -u\n",
        "  else: \n",
        "    !kaggle datasets create -p  /content/model\n",
        "\n",
        "def kaggle_updateDataset(commit_message='Updating files'):\n",
        "   !kaggle datasets version -p /content/model/ -m commit_message\n",
        "\n",
        "def kaggle_setConfig(dataset_title=\"testing\"):\n",
        "\n",
        "  fileName=\"data/dataset-metadata.json\"\n",
        "  !kaggle datasets init -p  /content/model/\n",
        "  addTitle2JSON(fileName,dataset_title)\n",
        "\n",
        "\n",
        "def kaggle_saveModel():\n",
        "  \n",
        "  res = input(\"Do you want upload model  in  your kaggle Account? [y/n] \")\n",
        "  \n",
        "  if  res.lower()!=\"y\" :\n",
        "    return print(\"Upload canceled\")\n",
        "\n",
        "  res = readOption( msg = \"Create new dataset for saving models[1] or update old one[2]? [1/2]\")\n",
        "  \n",
        "  dataset_title = input(\"Enter dataset name: \")\n",
        "  assert len(res)>0 , \"Invalid length \"\n",
        "\n",
        "  kaggle_setConfig(dataset_title)\n",
        "\n",
        "  if res==\"1\": #Create new dataset\n",
        "    res = readOption( msg = \"Create dataset Private[1] or Public [2]? [1/2]\" )\n",
        "    public_dataset = True if int(res)==2 else False\n",
        "    kaggle_createDataset(public_dataset)\n",
        "  elif res==\"2\": #Update dataset\n",
        "    commit_msg = input (\"Enter commit message (Ex. 'version 1.4') : \")\n",
        "    kaggle_updateDataset(commit_msg)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiAiQAWZOiyd"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlytG-pKOjOI",
        "outputId": "b8ba29da-e705-4cab-f2ac-3a20cdfae62c"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXCcOK-24E9I"
      },
      "source": [
        "### Model Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ofw-k7E4Man"
      },
      "source": [
        "def loadModel(name):\n",
        "  try:\n",
        "    return load_model(name)\n",
        "  except:\n",
        "    raise Exception(\"Model not found\")  \n",
        "\n",
        "  \n",
        "\n",
        "def fetchModel():\n",
        "  print(bcolors.WARNING + \" Downloading models from Kaggle...\" + bcolors.ENDC)  \n",
        "\n",
        "  fetchModelsFromKaggle()\n",
        "  print(bcolors.OKGREEN + \" Models downloaded in your local folder\" + bcolors.ENDC)  \n",
        "  print(\"---------------------------------\")\n",
        "  model_name = input (\"\\nEnter model file_name: ( Ex: model_a.h5 ) \")\n",
        "  print(bcolors.WARNING + \" Searching model...\" + bcolors.ENDC)  \n",
        "\n",
        "  m = loadModel(model_name)\n",
        "  print(bcolors.OKGREEN + f\"Model {model_name[:-3]} loaded successfully \" + bcolors.ENDC)  \n",
        "  return m\n",
        "\n",
        "def getModel():\n",
        "\n",
        "  res = readOption(\" Create new model [1] or load pre-trained model from Kaggle [2]  [1/2] \")\n",
        "\n",
        "  if res =='1':\n",
        "    model_name = input(\" Set  model name: (Ex. model_A) \")\n",
        "    model = createModel( name= model_name ,num_breeds = 10 )\n",
        "    print(bcolors.OKGREEN + f\"Model {model_name[:-3]} created successfully \" + bcolors.ENDC)  \n",
        "  else:\n",
        "    if checkIfFileExist('kaggle.json')==False:\n",
        "      loadCredentials()\n",
        "    model = fetchModel();\n",
        "  return model\n",
        "\n",
        "def model2TFLite(model):\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "  tfmodel = converter.convert()\n",
        "  open(model.name+'.tflite','wb').write(tfmodel)\n",
        "  print(\"Model successfully convert to .tflite\")\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV6MPr7R4OVA"
      },
      "source": [
        "### Extra Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNqZSpmt4R7-"
      },
      "source": [
        "def readOption(msg):\n",
        "\n",
        "  res = input(msg)\n",
        "  assert res=='1' or res=='2',\"Invalid option\"\n",
        "  return res\n",
        "\n",
        "def decode_breedID(idx):\n",
        "  df_decode = pd.read_csv('decode_breed.csv')\n",
        "  breed_encode = df_decode['breed_encode']\n",
        "  return df_decode.loc[breed_encode == idx]['breed'].tolist()[0]\n",
        "\n",
        "def custom_preprocess_input(x):\n",
        "\n",
        "\n",
        "  if not issubclass(x.dtype.type, np.floating):\n",
        "    x = x.astype('float32', copy=False)\n",
        "  print(x.dtype.type)\n",
        "\n",
        "      # 'RGB'->'BGR'\n",
        " # x = x[..., ::-1]\n",
        "  mean = [103.939, 116.779, 123.68]\n",
        "  # Zero-center by mean pixel\n",
        "  x[..., 0] -= mean[0]\n",
        "  x[..., 1] -= mean[1]\n",
        "  x[..., 2] -= mean[2]\n",
        "\n",
        "  return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvEW2XFL6lai"
      },
      "source": [
        "### Training Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttJ8x1Gj6ncD"
      },
      "source": [
        "def train(model,x_train,y_train,x_test,y_test , batch_size, epochs, model_name=\"\"):\n",
        "    model.fit(x_train, y_train, epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(x_test, y_test))\n",
        "    metrics = model.evaluate(x_test, y_test)\n",
        "    return {k:v for k,v in zip (model.metrics_names, metrics)}\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_K-R-ClGS2j"
      },
      "source": [
        "### File Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6lGv4PiGUYE"
      },
      "source": [
        "def addTitle2JSON(fileName,dataset_title):\n",
        "    data = readFile(fileName)\n",
        "    data = addTitle2Data(data,dataset_title)\n",
        "    writeFile(fileName,data)\n",
        "    \n",
        "def writeFile(fileName,data):\n",
        "  with open(fileName,'w') as json_file:\n",
        "      # Guardar la informacion\n",
        "      json.dump(data,json_file)\n",
        "      print(\"File \"+fileName+\" updated\")\n",
        "\n",
        "def readFile(fileName):\n",
        "  with open(fileName, \"r\") as json_file: \n",
        "     return json.load(json_file)\n",
        "\n",
        "def addTitle2Data(data,title):\n",
        "  data['id'] = data['id'].split('/')[0]+'/'+title\n",
        "  data['title'] = title\n",
        "  return data\n",
        "\n",
        "def checkIfFileExist(file_name):\n",
        "    return os.path.exists(os.path.abspath(file_name))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzaK32JPKN2i"
      },
      "source": [
        "## Convolutional network arquitecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XX_H1OitKRa"
      },
      "source": [
        "- Using VGG16 Arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-78on5b8s5sB"
      },
      "source": [
        "#Custom model\n",
        "def createModel(num_breeds,name=\"model\", ) :\n",
        "\n",
        "  vgg= VGG16( \n",
        "      input_shape= (224,224,3), \n",
        "      weights='imagenet',\n",
        "      include_top=False\n",
        "      )\n",
        "\n",
        "  #vgg.summary()\n",
        "  custom_model=vgg\n",
        "\n",
        "  for layer in custom_model.layers:\n",
        "    layer.trainable = False\n",
        "      \n",
        "  last_layer = Flatten()(custom_model.output)\n",
        "  last_layer = Dense(128, activation='relu', name='full_connected_1')(last_layer)\n",
        "  last_layer = Dense(128, activation='relu', name='full_connected_2')(last_layer)\n",
        "  out = Dense(num_breeds, activation='softmax', name='output')(last_layer)\n",
        "  custom_model = Model(custom_model.input, out,name=name)\n",
        "\n",
        "  \n",
        "  opt = Adam(lr=0.001) #Stochastic gradient descent\n",
        "  custom_model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  return custom_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJlXtxzf49zH"
      },
      "source": [
        "- Create a new model or load pre-trained model from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlUboEsAF5dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c1778c-f96f-45a5-8362-55a67ed4960d"
      },
      "source": [
        "model = getModel()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Create new model [1] or load pre-trained model from Kaggle [2]  [1/2] 2\n",
            "\u001b[93m Downloading models from Kaggle...\u001b[0m\n",
            "Downloading snake-classifier-pretrainedmodels.zip to /content\n",
            " 80% 65.0M/81.0M [00:00<00:00, 71.4MB/s]\n",
            "100% 81.0M/81.0M [00:00<00:00, 137MB/s] \n",
            "Archive:  snake-classifier-pretrainedmodels.zip\n",
            "replace baseline_model.h5? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[92m Models downloaded in your local folder\u001b[0m\n",
            "---------------------------------\n",
            "\n",
            "Enter model file_name: ( Ex: model_a.h5 ) baseline_model.h5\n",
            "\u001b[93m Searching model...\u001b[0m\n",
            "\u001b[92mModel baseline_model loaded successfully \u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqzALibl5YP7"
      },
      "source": [
        "- Model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8THxTrt5X1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c60871b-9434-4152-d125-11c46cfebe03"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "full_connected_1 (Dense)     (None, 128)               3211392   \n",
            "_________________________________________________________________\n",
            "full_connected_2 (Dense)     (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 17,943,882\n",
            "Trainable params: 3,229,194\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSrJdh1-HhJn"
      },
      "source": [
        "## Training and validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jfSfnP_5ri2"
      },
      "source": [
        "- Fetch dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBxLhiex6Atl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167c6b56-b4d8-419f-b4c2-013100b5d92e"
      },
      "source": [
        "fetchDatasetFromKaggle( 'snake-breeds')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92mData from kaggle successfully fetch\n",
            "\u001b[0m\n",
            "\u001b[1mUnzipping data... \n",
            "\u001b[0m\n",
            "Archive:  snake-breeds.zip\n",
            "  inflating: group_1.h5              \n",
            "  inflating: group_10.h5             \n",
            "  inflating: group_11.h5             \n",
            "  inflating: group_12.h5             \n",
            "  inflating: group_13.h5             \n",
            "  inflating: group_14.h5             \n",
            "  inflating: group_15.h5             \n",
            "  inflating: group_16.h5             \n",
            "  inflating: group_17.h5             \n",
            "  inflating: group_18.h5             \n",
            "  inflating: group_19.h5             \n",
            "  inflating: group_2.h5              \n",
            "  inflating: group_20.h5             \n",
            "  inflating: group_21.h5             \n",
            "  inflating: group_22.h5             \n",
            "  inflating: group_23.h5             \n",
            "  inflating: pre_proccessed_data.csv  \n",
            "\u001b[92mData is ready in your local folder!\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYkoPG6v6BD9"
      },
      "source": [
        "- Training and validate just certain amout of groups because of RAM limits\n",
        "\n",
        "In this case is training just with eight groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo6iFOE1HjSc",
        "outputId": "92a51658-8cf2-4b22-8cfb-17b497465348"
      },
      "source": [
        "\n",
        "for i in range(1,8):\n",
        "\n",
        "    destination_filepath = BASE_PATH+'/v6/batchs/'+'group_'+str(i)+'.h5'\n",
        "    with h5py.File(destination_filepath, \"r\") as f:\n",
        "        \n",
        "        print(\"Group:\",i)\n",
        "        \n",
        "        x = f[\"input_data\"][:]\n",
        "        y = f[\"input_labels\"][:]\n",
        "        x=x.reshape(x.shape[0],224,224,3)\n",
        "        x=preprocess_input(x*255)\n",
        "        \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20)\n",
        "    \n",
        "    #Displaying information just for the first group\n",
        "    if i ==1 :\n",
        "      print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "      print(\"\\nDistribution of train classes\")\n",
        "      print(pd.Series(y_train).value_counts())\n",
        "      print(\"\\nDistribution of test classes\")\n",
        "      print(pd.Series(y_test).value_counts())\n",
        "\n",
        "    train(model,x_train,y_train,x_test,y_test , batch_size=32, epochs=10, model_name=\"modelA\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Group: 1\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 47s 220ms/step - loss: 7.5805 - accuracy: 0.2200 - val_loss: 3.8831 - val_accuracy: 0.2700\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 8s 154ms/step - loss: 1.7991 - accuracy: 0.6006 - val_loss: 3.7275 - val_accuracy: 0.3625\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 8s 155ms/step - loss: 0.5854 - accuracy: 0.8263 - val_loss: 3.8327 - val_accuracy: 0.3875\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 8s 156ms/step - loss: 0.2718 - accuracy: 0.9319 - val_loss: 4.3198 - val_accuracy: 0.3875\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 8s 160ms/step - loss: 0.1061 - accuracy: 0.9787 - val_loss: 4.3014 - val_accuracy: 0.3775\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 8s 163ms/step - loss: 0.0311 - accuracy: 0.9969 - val_loss: 4.2251 - val_accuracy: 0.3825\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 165ms/step - loss: 0.0175 - accuracy: 0.9987 - val_loss: 4.1393 - val_accuracy: 0.3875\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 4.1845 - val_accuracy: 0.4025\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 4.2325 - val_accuracy: 0.3900\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 4.2689 - val_accuracy: 0.3925\n",
            "13/13 [==============================] - 2s 133ms/step - loss: 4.2689 - accuracy: 0.3925\n",
            "Group: 2\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 1.3192 - accuracy: 0.7950 - val_loss: 2.3589 - val_accuracy: 0.6450\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 1.0597 - accuracy: 0.7919 - val_loss: 2.2342 - val_accuracy: 0.6275\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 177ms/step - loss: 0.6540 - accuracy: 0.8694 - val_loss: 2.2552 - val_accuracy: 0.6525\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.2752 - accuracy: 0.9300 - val_loss: 2.5829 - val_accuracy: 0.6350\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 176ms/step - loss: 0.2162 - accuracy: 0.9588 - val_loss: 2.3438 - val_accuracy: 0.6525\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.1648 - accuracy: 0.9669 - val_loss: 2.8346 - val_accuracy: 0.6550\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.0589 - accuracy: 0.9856 - val_loss: 2.5069 - val_accuracy: 0.6750\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0445 - accuracy: 0.9912 - val_loss: 2.9972 - val_accuracy: 0.6250\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0573 - accuracy: 0.9937 - val_loss: 2.7304 - val_accuracy: 0.6750\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.1387 - accuracy: 0.9775 - val_loss: 3.6773 - val_accuracy: 0.6100\n",
            "13/13 [==============================] - 2s 125ms/step - loss: 3.6773 - accuracy: 0.6100\n",
            "Group: 3\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.9394 - accuracy: 0.8894 - val_loss: 0.8426 - val_accuracy: 0.8725\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 174ms/step - loss: 0.4939 - accuracy: 0.9156 - val_loss: 0.8914 - val_accuracy: 0.8650\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 178ms/step - loss: 0.1515 - accuracy: 0.9625 - val_loss: 1.2577 - val_accuracy: 0.8475\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 178ms/step - loss: 0.0966 - accuracy: 0.9812 - val_loss: 0.8677 - val_accuracy: 0.8850\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 176ms/step - loss: 0.0636 - accuracy: 0.9894 - val_loss: 1.0874 - val_accuracy: 0.8625\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.0698 - accuracy: 0.9875 - val_loss: 1.6276 - val_accuracy: 0.8425\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.1923 - accuracy: 0.9712 - val_loss: 1.3283 - val_accuracy: 0.8325\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0745 - accuracy: 0.9844 - val_loss: 1.5384 - val_accuracy: 0.8375\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.1172 - accuracy: 0.9794 - val_loss: 1.7132 - val_accuracy: 0.8125\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.1212 - accuracy: 0.9775 - val_loss: 1.4250 - val_accuracy: 0.8200\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 1.4250 - accuracy: 0.8200\n",
            "Group: 4\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.4798 - accuracy: 0.9350 - val_loss: 0.8113 - val_accuracy: 0.9050\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 175ms/step - loss: 0.4061 - accuracy: 0.9381 - val_loss: 0.6635 - val_accuracy: 0.9050\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.3662 - accuracy: 0.9519 - val_loss: 0.9355 - val_accuracy: 0.8950\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.1764 - accuracy: 0.9613 - val_loss: 0.9413 - val_accuracy: 0.8825\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 176ms/step - loss: 0.0736 - accuracy: 0.9825 - val_loss: 1.0151 - val_accuracy: 0.9075\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 8s 171ms/step - loss: 0.0694 - accuracy: 0.9881 - val_loss: 1.1640 - val_accuracy: 0.8925\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.0687 - accuracy: 0.9887 - val_loss: 1.0396 - val_accuracy: 0.9025\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0278 - accuracy: 0.9962 - val_loss: 1.0641 - val_accuracy: 0.8900\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 1.0321 - val_accuracy: 0.8900\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.9424 - val_accuracy: 0.8625\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 0.9424 - accuracy: 0.8625\n",
            "Group: 5\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.2758 - accuracy: 0.9650 - val_loss: 0.3993 - val_accuracy: 0.9800\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.2292 - accuracy: 0.9606 - val_loss: 0.7165 - val_accuracy: 0.9175\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 178ms/step - loss: 0.2534 - accuracy: 0.9588 - val_loss: 0.4864 - val_accuracy: 0.9525\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.0964 - accuracy: 0.9831 - val_loss: 0.5071 - val_accuracy: 0.9625\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 177ms/step - loss: 0.0277 - accuracy: 0.9912 - val_loss: 0.5955 - val_accuracy: 0.9500\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.0412 - accuracy: 0.9912 - val_loss: 0.4635 - val_accuracy: 0.9600\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.4292 - val_accuracy: 0.9675\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.4486 - val_accuracy: 0.9625\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 2.0731e-04 - accuracy: 1.0000 - val_loss: 0.4548 - val_accuracy: 0.9650\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 5.3606e-05 - accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.9650\n",
            "13/13 [==============================] - 2s 129ms/step - loss: 0.4539 - accuracy: 0.9650\n",
            "Group: 6\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.1370 - accuracy: 0.9881 - val_loss: 0.1539 - val_accuracy: 0.9600\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.0369 - accuracy: 0.9900 - val_loss: 0.2724 - val_accuracy: 0.9900\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 177ms/step - loss: 0.1009 - accuracy: 0.9819 - val_loss: 0.5235 - val_accuracy: 0.9750\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.3930 - accuracy: 0.9563 - val_loss: 0.8415 - val_accuracy: 0.9250\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 178ms/step - loss: 0.5122 - accuracy: 0.9581 - val_loss: 1.0786 - val_accuracy: 0.9175\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.2868 - accuracy: 0.9625 - val_loss: 1.0574 - val_accuracy: 0.8925\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 170ms/step - loss: 0.1384 - accuracy: 0.9812 - val_loss: 0.8106 - val_accuracy: 0.9300\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.0445 - accuracy: 0.9900 - val_loss: 0.9067 - val_accuracy: 0.9425\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.1088 - accuracy: 0.9881 - val_loss: 1.0320 - val_accuracy: 0.9400\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.1132 - accuracy: 0.9937 - val_loss: 1.0180 - val_accuracy: 0.8825\n",
            "13/13 [==============================] - 2s 127ms/step - loss: 1.0180 - accuracy: 0.8825\n",
            "Group: 7\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.3448 - accuracy: 0.9681 - val_loss: 0.1809 - val_accuracy: 0.9725\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 9s 173ms/step - loss: 0.3542 - accuracy: 0.9769 - val_loss: 0.4747 - val_accuracy: 0.9525\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 9s 178ms/step - loss: 0.1891 - accuracy: 0.9781 - val_loss: 0.2763 - val_accuracy: 0.9500\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 9s 179ms/step - loss: 0.2004 - accuracy: 0.9775 - val_loss: 0.4535 - val_accuracy: 0.9450\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 9s 176ms/step - loss: 0.0727 - accuracy: 0.9862 - val_loss: 0.6188 - val_accuracy: 0.9425\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 9s 172ms/step - loss: 0.1383 - accuracy: 0.9844 - val_loss: 0.6225 - val_accuracy: 0.9325\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 8s 169ms/step - loss: 0.1275 - accuracy: 0.9894 - val_loss: 0.5613 - val_accuracy: 0.9425\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0581 - accuracy: 0.9906 - val_loss: 0.6095 - val_accuracy: 0.9375\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 8s 168ms/step - loss: 0.0414 - accuracy: 0.9925 - val_loss: 0.6840 - val_accuracy: 0.9175\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 8s 167ms/step - loss: 0.0932 - accuracy: 0.9931 - val_loss: 1.4348 - val_accuracy: 0.9200\n",
            "13/13 [==============================] - 2s 128ms/step - loss: 1.4348 - accuracy: 0.9200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwIDlMpM7uNH"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtPo4WTHHrU5"
      },
      "source": [
        "-Testing with images from google"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyieXRZN7wqh"
      },
      "source": [
        "\n",
        "#Set random image\n",
        "img = cv2.imread('agkistrodon-contortrix.png')\n",
        "\n",
        "img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC) #Resize\n",
        "plt.imshow(img) #Display Image\n",
        "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "img= img.reshape(1,224,224,3)\n",
        "img = custom_preprocess_input(img)\n",
        "yhat = model.predict(img)\n",
        "\n",
        "index = yhat.argmax()\n",
        "print(decode_breedID( index ))\n",
        "np.max(yhat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjsixfxdHfXk"
      },
      "source": [
        "## Saving model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuGYWhSx7LwP"
      },
      "source": [
        "- Local save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBb4yC5VFqQ_"
      },
      "source": [
        "#Saving model\n",
        "!mkdir model\n",
        "model.save('model/'model.name+\".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFUhJOOZ7i9p"
      },
      "source": [
        "- Saving model in Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JgUx2bWac6-",
        "outputId": "9d3965c2-7cd2-464c-d26d-4a2faf0fc0aa"
      },
      "source": [
        "kaggle_saveModel()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Do you want upload model  in  your kaggle Account? [y/n] n\n",
            "Upload canceled\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swAVSm2k7893"
      },
      "source": [
        "#Convert model to Tensorflow Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIlKCA_GB3E5",
        "outputId": "9cae151c-4604-4e49-dee8-25bdf6f2cf29"
      },
      "source": [
        "model2TFLite(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxunj5ubq/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxunj5ubq/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71785940"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}